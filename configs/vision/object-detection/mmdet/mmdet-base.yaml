const:
  num_gpus: "len({trainer.gpus})"
  samples_per_gpu: 2  # NOTE this is train batch size!!
  val_samples_per_gpu: 1  # NOTE this is val batch size!!

debug:
  view_train_augmentation:
    is_xywh: False
    bbox_unnormalization: False
    preprocess_f: convert-mmdetbbox

dataloader:
  base_dataloader: # depends on hardware:)
    sampler: null
    batch_sampler: null
    num_workers: "2*{const.num_gpus}"
    pin_memory: True
    collate_fn:
      name: mmcv_parallel_collate
  trn:
    batch_size: "{const.samples_per_gpu}*{const.num_gpus}"
    shuffle: True
    collate_fn:
      args:
        samples_per_gpu: "{const.samples_per_gpu}"
    # Extend default_collate to add support for:type:`~mmcv.parallel.DataContainer`.
  val:
    batch_size: "{const.val_samples_per_gpu}*{const.num_gpus}"
    shuffle: False
    collate_fn:
      args:
        samples_per_gpu: "{const.val_samples_per_gpu}"
  test:
    batch_size: "{const.val_samples_per_gpu}*{const.num_gpus}"
    shuffle: False
    collate_fn:
      args:
        samples_per_gpu: "{const.val_samples_per_gpu}"
